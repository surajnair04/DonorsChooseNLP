{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Set GPU flag to false if running on CPU\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(include_lengths=True)\n",
    "ID = data.Field(sequential=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "def sort_key(ex):\n",
    "    return len(ex.project_title)\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',\n",
    "        validation='val.csv', test='dev.csv', format='csv', skip_header=True,\n",
    "        fields=[('id', ID), ('project_title', TEXT),('project_resource_summary', None), \n",
    "                ('project_essay_1', TEXT), ('project_essay_2', TEXT), ('project_is_approved', LABEL)])\n",
    "\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).  If not interested, remove \"vectors\"\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "ID.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "#change device to 0 for GPU\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, val, test), sort_key= sort_key, repeat=False,\n",
    "        batch_size=(64), device=-1 if GPU else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive LSTM/BiLSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(vocab.vectors)        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(2*hidden_dim, label_size)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        if(GPU):\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "        \n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            \n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        return y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def test_result(net,iter_obj):\n",
    "    pred = []\n",
    "    actual = []\n",
    "    for batch in iter_obj:\n",
    "        input,label = batch.project_title[0], batch.project_is_approved-1\n",
    "        if(GPU): input = input.cuda()\n",
    "        net.hidden = net.init_hidden(input.shape[1])\n",
    "        scores = net(input)\n",
    "        pred.extend(scores.cpu().data.numpy().argmax(axis=1))\n",
    "        actual.extend(label.data.cpu().numpy().tolist())\n",
    "    return round(accuracy_score(actual,pred),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Naive BiLSTM model\n",
    "- Input: project_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 10 epochs\n",
      "Train accuracy: 0.9, Validation accuracy: 0.53\n",
      "End of 20 epochs\n",
      "Train accuracy: 0.96, Validation accuracy: 0.53\n",
      "End of 30 epochs\n",
      "Train accuracy: 0.97, Validation accuracy: 0.53\n",
      "End of 40 epochs\n",
      "Train accuracy: 0.97, Validation accuracy: 0.54\n",
      "End of 50 epochs\n",
      "Train accuracy: 0.97, Validation accuracy: 0.53\n",
      "End of 60 epochs\n",
      "Train accuracy: 0.97, Validation accuracy: 0.53\n",
      "End of 70 epochs\n",
      "Train accuracy: 0.98, Validation accuracy: 0.53\n",
      "End of 80 epochs\n",
      "Train accuracy: 0.98, Validation accuracy: 0.53\n",
      "End of 90 epochs\n",
      "Train accuracy: 0.98, Validation accuracy: 0.53\n",
      "End of 100 epochs\n",
      "Train accuracy: 0.98, Validation accuracy: 0.53\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import time\n",
    "NUM_EPOCHS = 100\n",
    "model = LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "                       hidden_dim=50, batch_size=64, label_size=2)\n",
    "\n",
    "\n",
    "\n",
    "if(GPU): model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_l = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    start = time.time()\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()       \n",
    "        input,label = batch.project_title[0], batch.project_is_approved-1\n",
    "        if(GPU): input,label = input.cuda(),label.cuda()\n",
    "        model.hidden = model.init_hidden(input.shape[1])\n",
    "\n",
    "        \n",
    "        scores = model(input)\n",
    "        loss = loss_function(scores, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_l.append(loss.cpu().data.numpy())\n",
    "    if((i+1)%10==0):\n",
    "        print(f\"End of {i+1} epochs\")\n",
    "        print(f\"Train accuracy: {test_result(model,train_iter)}, Validation accuracy: {test_result(model,val_iter)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
